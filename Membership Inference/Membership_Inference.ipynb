{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Membership_Inference.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9f1a762435b84589afc2601c22b01884": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_18637f8e01164a40af0eabeb96c02b96",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d2e7cb2aed9d4b16ad405a2bbd5da03f",
              "IPY_MODEL_9a8dc19361454bd99af42a1dcf8d84d5"
            ]
          }
        },
        "18637f8e01164a40af0eabeb96c02b96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d2e7cb2aed9d4b16ad405a2bbd5da03f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_192e50c3a0fe4cd09176a18257b03bee",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_89ec2871e5bb418582fbe7b020a5a1ee"
          }
        },
        "9a8dc19361454bd99af42a1dcf8d84d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0da5b87eb3e2470b9bcb7078d6fd9930",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:20&lt;00:00, 50949949.84it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_971651a4b1e140b6ab0b99856d3f810a"
          }
        },
        "192e50c3a0fe4cd09176a18257b03bee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "89ec2871e5bb418582fbe7b020a5a1ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0da5b87eb3e2470b9bcb7078d6fd9930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "971651a4b1e140b6ab0b99856d3f810a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWvwJRBluTYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split, ConcatDataset\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import os\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZQvU4ITOyt5",
        "colab_type": "text"
      },
      "source": [
        "### Process Image Dataset to be trained by CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YfnViYrvTm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_image_dataset(dataset, image_batch_size):\n",
        "    \n",
        "    if dataset == 'CIFAR10':\n",
        "        # Normalize dataset\n",
        "        transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "        \n",
        "        trainset = torchvision.datasets.CIFAR10(root='./data/CIFAR10', train=True,\n",
        "                                            download=True, transform=transform)\n",
        "\n",
        "\n",
        "        testset = torchvision.datasets.CIFAR10(root='./data/CIFAR10', train=False,\n",
        "                                               download=True, transform=transform)\n",
        "        \n",
        "    elif  dataset == 'MNIST':\n",
        "        # Normalize dataset\n",
        "        transform = transforms.Compose(\n",
        "             [transforms.ToTensor(),\n",
        "              transforms.Normalize((0.5,), (0.5,))]\n",
        "         )\n",
        "                \n",
        "        trainset = torchvision.datasets.MNIST(root='./data/MNIST', train=True,\n",
        "                                            download=True, transform=transform)\n",
        "\n",
        "\n",
        "        testset = torchvision.datasets.MNIST(root='./data/MNIST', train=False,\n",
        "                                               download=True, transform=transform)\n",
        "        \n",
        "    else:\n",
        "        transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "         transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "        trainset = torchvision.datasets.FashionMNIST(root='./data', train=True,\n",
        "                                            download=True, transform=transform)\n",
        "    \n",
        "\n",
        "\n",
        "        testset = torchvision.datasets.FashionMNIST(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "   \n",
        "    datasets=[]\n",
        "    datasets.append(trainset)\n",
        "    datasets.append(testset)\n",
        "    # Conact all data samples together different train and test set for image classification is not needed \n",
        "    fulldataset = ConcatDataset(datasets)\n",
        "    \n",
        "    #All the names of dataset after split is maintained exactly with Ahmed's paper\n",
        "    \n",
        "    Dshadow, Dtarget = random_split(fulldataset, (int(len(fulldataset)/2), int(len(fulldataset)/2)))\n",
        "    Dtrainshadow, Doutshadow = random_split(Dshadow, (int(len(fulldataset)/4), int(len(fulldataset)/4)))\n",
        "    Dtrain, Dnonmember = random_split(Dtarget, (int(len(fulldataset)/4), int(len(fulldataset)/4)))\n",
        "    Dtrainloader = torch.utils.data.DataLoader(Dtrain, image_batch_size,\n",
        "                                              shuffle=True, num_workers=0)\n",
        "    \n",
        "    Dnonmemberloader = torch.utils.data.DataLoader(Dnonmember, image_batch_size,\n",
        "                                              shuffle=True, num_workers=0)\n",
        "    \n",
        "    Dtrainshadowloader = torch.utils.data.DataLoader(Dtrainshadow, image_batch_size,\n",
        "                                              shuffle=True, num_workers=0)\n",
        "    \n",
        "    Doutshadowloader = torch.utils.data.DataLoader(Doutshadow, image_batch_size,\n",
        "                                              shuffle=True, num_workers=0) \n",
        "    \n",
        "    Dshadowloader = torch.utils.data.DataLoader(Dshadow, image_batch_size,\n",
        "                                              shuffle=True, num_workers=0) \n",
        "    \n",
        "    Dtargetloader = torch.utils.data.DataLoader(Dtarget, image_batch_size,\n",
        "                                              shuffle=True, num_workers=0) \n",
        "    \n",
        "    return Dtrainloader, Dnonmemberloader,  Dtrainshadowloader, Doutshadowloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lexzfBBgPtB_",
        "colab_type": "text"
      },
      "source": [
        "### Dataset class for feature vector of MLP(ATTACK model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxH9TMrGvWNE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "      'Characterizes a dataset for PyTorch'\n",
        "      def __init__(self, features, labels):\n",
        "            'Initialization'\n",
        "            self.features = features\n",
        "            self.labels = labels\n",
        "    \n",
        "      def __len__(self):\n",
        "            'Denotes the total number of samples'\n",
        "            return len(self.labels)\n",
        "    \n",
        "      def __getitem__(self, index):\n",
        "            'Generates one sample of data'\n",
        "            # Select sample\n",
        "    \n",
        "            # Load data and get label\n",
        "            X = self.features[index]\n",
        "            y = self.labels[index]\n",
        "    \n",
        "            return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgOoXbBePwvp",
        "colab_type": "text"
      },
      "source": [
        "### Process Posterior Dataset to be Trained by MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG83Uoe2vceS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_posterior_dataset(model, Dmemberloader, Dnonmemberloader, posterior_batch_size, image_batch_size, shuffle):\n",
        "    \n",
        "    #mlp_X will be prepared for feeding in attack model that contains three top posteriors\n",
        "    #mlp_Y will contain the label 1-member/0-nonmember\n",
        "    #Dataset will be Shuffled (mandatory) for shadow posteriors but for making target posteriors it will be set to false as, for target we need just the prediction we won't train attack model on target data\n",
        "    mlp_X = torch.empty(0,3).to(torch.float32)\n",
        "    mlp_one = torch.ones(len(Dmemberloader)*image_batch_size,1, dtype=int).to(torch.float32)\n",
        "    mlp_zero = torch.zeros(len(Dnonmemberloader)*image_batch_size,1, dtype=int).to(torch.float32)\n",
        "    mlp_Y = torch.cat((mlp_one,mlp_zero),0)\n",
        "   \n",
        "    #Get all the posteriors from already trained model, rank them high to low and make feature vector mlp_X \n",
        "    with torch.no_grad():\n",
        "            for data in Dmemberloader:\n",
        "                images, labels = data\n",
        "                outputs = model(images)\n",
        "                \n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                c = (predicted == labels).squeeze()\n",
        "                for i in range(image_batch_size):\n",
        "                    values, _ = torch.sort(outputs[i])\n",
        "                    mlp_X = torch.cat((torch.flip(values[-3:],[0]).view(1,3), mlp_X))\n",
        "                    \n",
        "            for data in Dnonmemberloader:\n",
        "              \n",
        "                images, labels = data\n",
        "                outputs = model(images)\n",
        "                \n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                c = (predicted == labels).squeeze()\n",
        "                for i in range(image_batch_size):\n",
        "                    values, _ = torch.sort(outputs[i])\n",
        "                    mlp_X = torch.cat((torch.flip(values[-3:],[0]).view(1,3), mlp_X))\n",
        "           \n",
        "    mlp_X = torch.flip(mlp_X,[0])\n",
        "    \n",
        "    mlp_dataset = Dataset(mlp_X, mlp_Y)\n",
        "\n",
        "    Dposteriorloader = torch.utils.data.DataLoader(mlp_dataset, posterior_batch_size,\n",
        "                                      shuffle=shuffle)\n",
        "        \n",
        "    return Dposteriorloader "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7ZdpX8EP744",
        "colab_type": "text"
      },
      "source": [
        "### Attack Network for MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj6NU07gveBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Attack network\n",
        "class attack_net(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(attack_net, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size  = hidden_size\n",
        "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.fc2 = torch.nn.Linear(self.hidden_size, 1)\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "        # self.softmax = torch.nn.Softmax()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        hidden = self.fc1(x)\n",
        "        relu = self.relu(hidden)\n",
        "        output = self.fc2(relu)\n",
        "        output = self.sigmoid(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOHhZDODQI9s",
        "colab_type": "text"
      },
      "source": [
        "### Train Attack Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EtqqUqhvlPS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_attack_model(attack_data_loader, input_size, hidden_size, lr, epochs, batch_size):\n",
        "  attack_model = attack_net(input_size, hidden_size)\n",
        "  \n",
        "  criterion = nn.MSELoss()\n",
        "  optimizer_attack = torch.optim.ASGD(attack_model.parameters(), lr)\n",
        "  \n",
        "  print('Training the Attack Model...')\n",
        "  for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    for i, data in enumerate(attack_data_loader, 0):\n",
        "        \n",
        "        inputs, labels = data\n",
        "        \n",
        "        optimizer_attack.zero_grad()\n",
        "        # Forward pass\n",
        "        outputs = attack_model(inputs)\n",
        "        # Compute Loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "      \n",
        "      \n",
        "        optimizer_attack.step()\n",
        "        \n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 3000 == 2999:    # print every 1500 mini-batches\n",
        "            print('[Epoch %d, Batch %1d] loss: %.6f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 3000))\n",
        "            running_loss = 0.0\n",
        "                      \n",
        "  print('Finished Training Attack Model')\n",
        "  #torch.save(attack_model.state_dict(), PATH) #you can also save the attack model if PATH is added as param\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for data in attack_data_loader:\n",
        "      X, y = data\n",
        "      outputs = attack_model(X).round()\n",
        "      total += batch_size\n",
        "      correct += (outputs == y).sum().item()\n",
        "\n",
        "\n",
        "  print('Accuracy of the attack on the All Shadow(Dshadow) Data: %d %%' % (\n",
        "  100 * correct / total))\n",
        "  return attack_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlKF0TIGQQZV",
        "colab_type": "text"
      },
      "source": [
        "### Network for training CIFAR10 target model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lru4elgSvpX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class CIFAR_target_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CIFAR_target_Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, (5,5), padding=2)\n",
        "        self.conv2 = nn.Conv2d(32, 32, (5,5))\n",
        "        self.fc1   = nn.Linear(32*6*6, 128)\n",
        "        self.fc2   = nn.Linear(128, 10)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mxvxsJmQkzc",
        "colab_type": "text"
      },
      "source": [
        "### Network for training MNIST/FashionMNIST target model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZiQEMz1QhBC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class target_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(target_Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16,5)\n",
        "        self.fc1   = nn.Linear(256, 128)\n",
        "        self.fc2   = nn.Linear(128, 10)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x       \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRXQ0qobQzZM",
        "colab_type": "text"
      },
      "source": [
        "### Train Target/Victim Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqwZMq-RQxF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_target_model(dataset, Dtrainloader, Dnonmemberloader, lr, momentum, num_epochs):\n",
        "    net = target_Net()\n",
        "    if dataset == 'CIFAR10':\n",
        "        net = CIFAR_target_Net() \n",
        "      \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer_target = optim.SGD(net.parameters(), lr, momentum)\n",
        "    \n",
        "    print('Training the Target Model...')\n",
        "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(Dtrainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "    \n",
        "            # zero the parameter gradients\n",
        "            optimizer_target.zero_grad()\n",
        "    \n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_target.step()\n",
        "    \n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "                print('[Epoch %d, Batch %3d] loss: %.3f' %\n",
        "                      (epoch + 1, i + 1, running_loss / 2000))\n",
        "                running_loss = 0.0\n",
        "\n",
        "    print('Finished Training Traget Model')\n",
        "    \n",
        "      \n",
        "    #Check accuracy in the training data\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in Dtrainloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    print('Accuracy of the prediction of image classification in training(member) data: %d %%' % (\n",
        "        100 * correct / total))\n",
        "    \n",
        "    #Check accuracy in the samples which were not seen during training\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in Dnonmemberloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    print('Accuracy of the prediction of image classification in remaining nonmember data : %d %%' % (\n",
        "        100 * correct / total))\n",
        "    \n",
        "    return net #Returns the trained target model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2gAIHDRQ4NU",
        "colab_type": "text"
      },
      "source": [
        "### Train Shadow Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1orTUb8-vueq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_shadow_model(dataset, Dtrainshadowloader, Doutshadowloader, lr, momentum, num_epochs):\n",
        "    net_shadow = target_Net()\n",
        "    \n",
        "    if dataset == 'CIFAR10':\n",
        "         net_shadow = CIFAR_target_Net()\n",
        "       \n",
        "       \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer_shadow = optim.SGD(net_shadow.parameters(), lr, momentum)\n",
        "  \n",
        "    print('Training the Shadow Model...')\n",
        "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(Dtrainshadowloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "    \n",
        "            # zero the parameter gradients\n",
        "            optimizer_shadow.zero_grad()\n",
        "    \n",
        "            # forward + backward + optimize\n",
        "            outputs = net_shadow(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_shadow.step()\n",
        "    \n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "                print('[Epoch %d, Batch %3d] loss: %.3f' %\n",
        "                      (epoch + 1, i + 1, running_loss / 2000))\n",
        "                running_loss = 0.0\n",
        "\n",
        "    print('Finished Training Shadow Model')\n",
        "       \n",
        "    \n",
        "    #Check accuracy in the training data\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in Dtrainshadowloader:\n",
        "            images, labels = data\n",
        "            outputs = net_shadow(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    print('Accuracy of the prediction of image classification in training shadow(Dtrainshadow) Data): %d %%' % (\n",
        "        100 * correct / total))\n",
        "    \n",
        "    #Check accuracy in the samples which were not seen during training\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in Doutshadowloader:\n",
        "            images, labels = data\n",
        "            outputs = net_shadow(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    print('Accuracy of the prediction of image classification in out shadow(Doutshadow) Data : %d %%' % (\n",
        "        100 * correct / total))\n",
        "    \n",
        "    return net_shadow #Returns the trained shadow model\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD7oIem0RFwl",
        "colab_type": "text"
      },
      "source": [
        "### Run Membership Inference "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbf-tkBExdT3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9f1a762435b84589afc2601c22b01884",
            "18637f8e01164a40af0eabeb96c02b96",
            "d2e7cb2aed9d4b16ad405a2bbd5da03f",
            "9a8dc19361454bd99af42a1dcf8d84d5",
            "192e50c3a0fe4cd09176a18257b03bee",
            "89ec2871e5bb418582fbe7b020a5a1ee",
            "0da5b87eb3e2470b9bcb7078d6fd9930",
            "971651a4b1e140b6ab0b99856d3f810a"
          ]
        },
        "outputId": "ad2c50f7-033b-46e1-ebf4-b47f6d185ae3"
      },
      "source": [
        "selected_dataset = 'CIFAR10' # change it to MNIST or FashionMNIST to run on these datasets \n",
        "\n",
        "# Preprocess dataset and split them\n",
        "image_batch_size=4\n",
        "Dtrainloader, Dnonmemberloader, Dtrainshadowloader, Doutshadowloader = process_image_dataset(selected_dataset,image_batch_size)\n",
        "\n",
        "#Train the TARGET model on CNN\n",
        "\n",
        "lr = 0.001\n",
        "momentum = 0.9\n",
        "epochs = 10\n",
        "target_model = train_target_model(selected_dataset, Dtrainloader, Dnonmemberloader, lr, momentum, epochs)\n",
        "\n",
        "#Train the SHADOW model on CNN\n",
        "shadow_model = train_shadow_model(selected_dataset, Dtrainshadowloader, Doutshadowloader, lr, momentum, epochs)\n",
        "\n",
        "#Process Posterior data for building ATTACK model\n",
        "#3 Highest Posteriors are used to Genearte feature vectors for Multilayer Perceptron (ATTACK model)\n",
        "#Posteriors are sorted from HIGH to LOW values as mentioned in the paper\n",
        "#mlp_X contains the feature tensors for tarining the Attack model\n",
        "#mlp_Y contains the corresponding labels (1-member/0-nonmember)\n",
        "posterior_batch_size = 4\n",
        "shuffle = True #This is mandatory to set to True\n",
        "attack_data_loader = process_posterior_dataset(shadow_model, Dtrainshadowloader, Doutshadowloader, posterior_batch_size, image_batch_size, shuffle)\n",
        "\n",
        "#Now feed these posteriors in attack_data_loader for training the ATTACK model which is built on MLP as per the paper\n",
        "input_size = 3\n",
        "hidden_size = 64\n",
        "epochs = 25\n",
        "lr = 1e-05\n",
        "attack_model = train_attack_model(attack_data_loader, input_size, hidden_size, lr, epochs, posterior_batch_size)\n",
        "\n",
        "#As attack_model is already trained on the whole shadow data(Dtrainshadow and Doutshadow)\n",
        "#The next step would be to get the posteriors for all the samples in target dataset(Dtarget-both members and non-member) using target model\n",
        "#target_X contains the posteriors for all target data that will be then checked for membership\n",
        "#target_Y contains the corresponding labels (1-member/0-nonmember)\n",
        "posterior_batch_size = 4\n",
        "shuffle = False  #This is mandatory to set to False for comparison purpose\n",
        "target_posteriors_loader = process_posterior_dataset(target_model, Dtrainloader, Dnonmemberloader, posterior_batch_size, image_batch_size, shuffle)\n",
        "\n",
        "#Now target dataset is ready with their posteriors to be fed in attack_model\n",
        "#Finally, let's get the Membership Inference for All the target Data\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "TP = 0\n",
        "FP = 0\n",
        "TN = 0\n",
        "FN = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate (target_posteriors_loader,0):\n",
        "          X, y = data\n",
        "          outputs = attack_model(X).round()\n",
        "          \n",
        "          #As target data is not shuffled later on we know that\n",
        "          #the first half target_posteriors_loader is member and the later half is non-member\n",
        "          #that is why i < len(Dtrainloader) is used for checking the prediction of class 1 and vice versa\n",
        "          #class/label 1 means member and vice versa\n",
        "          \n",
        "          if i < len(Dtrainloader):\n",
        "            TP += (outputs == 1).sum().item()\n",
        "            FN += (outputs == 0).sum().item()\n",
        "            \n",
        "          else:\n",
        "            FP += (outputs == 1).sum().item()\n",
        "            TN += (outputs == 0).sum().item()\n",
        "        \n",
        "    \n",
        "print('Accuracy of the attack on the all Target Data: %d %%' % (\n",
        "    100 * (TP+TN) / (len(target_posteriors_loader) * posterior_batch_size)))\n",
        "\n",
        "print('Precision of the attack on the all Target Data of class 1(member): %d %%' % (\n",
        "    100 * (TP / (TP+FP))))\n",
        "\n",
        "print('Recall of the attack on the all Target Data of class 1(member): %d %%' % (\n",
        "    100 * (TP / (TP+FN))))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/CIFAR10/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f1a762435b84589afc2601c22b01884",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/CIFAR10/cifar-10-python.tar.gz to ./data/CIFAR10\n",
            "Files already downloaded and verified\n",
            "Training the Target Model...\n",
            "[Epoch 1, Batch 2000] loss: 1.936\n",
            "[Epoch 2, Batch 2000] loss: 1.422\n",
            "[Epoch 3, Batch 2000] loss: 1.240\n",
            "[Epoch 4, Batch 2000] loss: 1.073\n",
            "[Epoch 5, Batch 2000] loss: 0.940\n",
            "Finished Training Traget Model\n",
            "Accuracy of the prediction of image classification in training(member) data: 69 %\n",
            "Accuracy of the prediction of image classification in remaining nonmember data : 57 %\n",
            "Training the Shadow Model...\n",
            "[Epoch 1, Batch 2000] loss: 1.955\n",
            "[Epoch 2, Batch 2000] loss: 1.469\n",
            "[Epoch 3, Batch 2000] loss: 1.239\n",
            "[Epoch 4, Batch 2000] loss: 1.065\n",
            "[Epoch 5, Batch 2000] loss: 0.930\n",
            "Finished Training Shadow Model\n",
            "Accuracy of the prediction of image classification in training shadow(Dtrainshadow) Data): 74 %\n",
            "Accuracy of the prediction of image classification in out shadow(Doutshadow) Data : 61 %\n",
            "Training the Attack Model...\n",
            "[Epoch 1, Batch 3000] loss: 0.253141\n",
            "[Epoch 1, Batch 6000] loss: 0.251746\n",
            "[Epoch 2, Batch 3000] loss: 0.251993\n",
            "[Epoch 2, Batch 6000] loss: 0.251071\n",
            "[Epoch 3, Batch 3000] loss: 0.251687\n",
            "[Epoch 3, Batch 6000] loss: 0.251269\n",
            "[Epoch 4, Batch 3000] loss: 0.251538\n",
            "[Epoch 4, Batch 6000] loss: 0.251538\n",
            "[Epoch 5, Batch 3000] loss: 0.250638\n",
            "[Epoch 5, Batch 6000] loss: 0.251809\n",
            "[Epoch 6, Batch 3000] loss: 0.251327\n",
            "[Epoch 6, Batch 6000] loss: 0.250945\n",
            "[Epoch 7, Batch 3000] loss: 0.251078\n",
            "[Epoch 7, Batch 6000] loss: 0.251044\n",
            "[Epoch 8, Batch 3000] loss: 0.250279\n",
            "[Epoch 8, Batch 6000] loss: 0.251860\n",
            "[Epoch 9, Batch 3000] loss: 0.250861\n",
            "[Epoch 9, Batch 6000] loss: 0.251186\n",
            "[Epoch 10, Batch 3000] loss: 0.251290\n",
            "[Epoch 10, Batch 6000] loss: 0.250515\n",
            "[Epoch 11, Batch 3000] loss: 0.250950\n",
            "[Epoch 11, Batch 6000] loss: 0.250427\n",
            "[Epoch 12, Batch 3000] loss: 0.250276\n",
            "[Epoch 12, Batch 6000] loss: 0.251411\n",
            "[Epoch 13, Batch 3000] loss: 0.250415\n",
            "[Epoch 13, Batch 6000] loss: 0.250863\n",
            "[Epoch 14, Batch 3000] loss: 0.250110\n",
            "[Epoch 14, Batch 6000] loss: 0.250411\n",
            "[Epoch 15, Batch 3000] loss: 0.249963\n",
            "[Epoch 15, Batch 6000] loss: 0.251358\n",
            "[Epoch 16, Batch 3000] loss: 0.250581\n",
            "[Epoch 16, Batch 6000] loss: 0.250115\n",
            "[Epoch 17, Batch 3000] loss: 0.250687\n",
            "[Epoch 17, Batch 6000] loss: 0.250376\n",
            "[Epoch 18, Batch 3000] loss: 0.250264\n",
            "[Epoch 18, Batch 6000] loss: 0.250663\n",
            "[Epoch 19, Batch 3000] loss: 0.250325\n",
            "[Epoch 19, Batch 6000] loss: 0.250129\n",
            "[Epoch 20, Batch 3000] loss: 0.250486\n",
            "[Epoch 20, Batch 6000] loss: 0.249894\n",
            "[Epoch 21, Batch 3000] loss: 0.250473\n",
            "[Epoch 21, Batch 6000] loss: 0.250202\n",
            "[Epoch 22, Batch 3000] loss: 0.249872\n",
            "[Epoch 22, Batch 6000] loss: 0.250375\n",
            "[Epoch 23, Batch 3000] loss: 0.250337\n",
            "[Epoch 23, Batch 6000] loss: 0.249832\n",
            "[Epoch 24, Batch 3000] loss: 0.250830\n",
            "[Epoch 24, Batch 6000] loss: 0.249387\n",
            "[Epoch 25, Batch 3000] loss: 0.250575\n",
            "[Epoch 25, Batch 6000] loss: 0.249923\n",
            "Finished Training Attack Model\n",
            "Accuracy of the attack on the All Shadow(Dshadow) Data: 51 %\n",
            "Accuracy of the attack on the all Target Data: 50 %\n",
            "Precision of the attack on the all Target Data of class 1(member): 50 %\n",
            "Recall of the attack on the all Target Data of class 1(member): 50 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzuOw_h0VgPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}